name: Daily Stock Screener (CAD) + Email

on:
  schedule:
    # Run 3x per WEEKDAY (Mon-Fri) for timely trading signals.
    # NYSE hours: 9:30 AM - 4:00 PM ET.
    # Times chosen so the email arrives with enough time to act in BOTH
    # winter (EST = UTC-5) and summer (EDT = UTC-4).
    #
    #                          EST (winter)     EDT (summer)
    # ─────────────────────────────────────────────────────────
    # PRE-MARKET  12:00 UTC    7:00 AM (-2.5h)  8:00 AM (-1.5h)
    # MID-DAY     15:00 UTC   10:00 AM (+0.5h) 11:00 AM (+1.5h)
    # PRE-CLOSE   19:30 UTC    2:30 PM (-1.5h)  3:30 PM (-0.5h)
    #
    # Pipeline takes ~5-10 min, so emails arrive well before the action window.

    # PRE-MARKET: 12:00 UTC — review + place orders before 9:30 AM open
    - cron: '0 12 * * 1-5'

    # MID-DAY: 15:00 UTC — after opening volatility settles (~30-90 min in)
    - cron: '0 15 * * 1-5'

    # PRE-CLOSE: 19:30 UTC — last chance for end-of-day positioning
    - cron: '30 19 * * 1-5'
  workflow_dispatch:

concurrency:
  group: daily-stock-screener
  cancel-in-progress: false

permissions:
  contents: read
  issues: write
  actions: read

jobs:
  daily_screener:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Start telemetry timer
        run: echo "WORKFLOW_START_TS=$(date +%s)" >> $GITHUB_ENV

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip
        id: cache_pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements-actions.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Restore universe/data/state cache
        id: cache_data_restore
        uses: actions/cache/restore@v4
        with:
          path: |
            cache/
            data_cache/
            screener_portfolio_state.json
            screener_portfolio_state.json.bak
          key: ${{ runner.os }}-daily-screener-data-${{ hashFiles('requirements-actions.txt', '.github/workflows/daily-stock-screener.yml') }}
          restore-keys: |
            ${{ runner.os }}-daily-screener-data-

      - name: Cache ML model
        id: cache_model
        uses: actions/cache@v4
        with:
          path: |
            models/
          key: ${{ runner.os }}-screener-model-${{ hashFiles('.github/workflows/train-stock-screener-model.yml') }}
          restore-keys: |
            ${{ runner.os }}-screener-model-

      - name: Find latest trained model artifact
        id: model_artifact
        uses: actions/github-script@v7
        with:
          script: |
            const { owner, repo } = context.repo;
            const workflow_id = "train-stock-screener-model.yml";
            const defaultBranch =
              context.payload.repository?.default_branch ||
              (context.ref ? String(context.ref).replace("refs/heads/", "") : "") ||
              "main";

            let runs = await github.paginate(github.rest.actions.listWorkflowRuns, {
              owner,
              repo,
              workflow_id,
              status: "success",
              branch: defaultBranch,
              per_page: 50,
            });

            // Fallback for repositories/events where branch filtering returns nothing.
            if (!runs.length) {
              runs = await github.paginate(github.rest.actions.listWorkflowRuns, {
                owner,
                repo,
                workflow_id,
                status: "success",
                per_page: 50,
              });
            }

            for (const run of runs) {
              const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                owner,
                repo,
                run_id: run.id,
              });
              const match = artifacts.data.artifacts.find(
                (artifact) => artifact.name.startsWith("screener-model-") && !artifact.expired
              );
              if (match) {
                core.setOutput("artifact_id", String(match.id));
                core.setOutput("artifact_name", match.name);
                core.setOutput("run_id", String(run.id));
                return;
              }
            }
            core.setOutput("artifact_id", "");
            core.setOutput("artifact_name", "");
            core.setOutput("run_id", "");

      - name: Download latest trained model
        if: steps.model_artifact.outputs.artifact_id != ''
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          echo "Found model: ${{ steps.model_artifact.outputs.artifact_name }} (run ${{ steps.model_artifact.outputs.run_id }})"
          rm -rf models/ensemble/

          # download-artifact@v4 cannot fetch artifacts from other workflow runs.
          # Use the REST API directly instead (gh cli handles auth + redirects).
          gh api "repos/${{ github.repository }}/actions/artifacts/${{ steps.model_artifact.outputs.artifact_id }}/zip" > /tmp/model.zip
          unzip -o /tmp/model.zip -d /tmp/model-extract

          # Handle both possible structures:
          #   a) artifact preserves workspace-relative path: models/ensemble/*
          #   b) artifact contains files at root: manifest.json, *.json, *.bin
          if [ -d "/tmp/model-extract/models/ensemble" ]; then
            mv /tmp/model-extract/models/ensemble models/ensemble
          elif [ -f "/tmp/model-extract/manifest.json" ]; then
            mkdir -p models/ensemble
            mv /tmp/model-extract/* models/ensemble/
          else
            echo "WARNING: Unexpected artifact structure:"
            find /tmp/model-extract -type f | head -20
            mkdir -p models/ensemble
          fi

          rm -rf /tmp/model.zip /tmp/model-extract
          echo "Model files downloaded:"
          ls -la models/ensemble/ 2>/dev/null || echo "No model files found"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-actions.txt

      - name: Ensure ML model exists (train if missing)
        shell: bash
        env:
          ALLOW_FALLBACK_TRAINING: "0"
          MODEL_PATH: "models/ensemble/manifest.json"
          LABEL_HORIZON_DAYS: "5"
          # Lightweight fallback — just enough for a usable model.
          MAX_TICKERS: "1500"
          FEATURE_LOOKBACK_DAYS: "365"
          BATCH_SIZE: "200"
          # Disable Optuna and use minimal ensemble for speed
          USE_OPTUNA: "0"
          ENSEMBLE_XGB_COUNT: "2"
          ENSEMBLE_LGBM_COUNT: "1"
          # Quality filters — must match daily screener and full training
          MIN_PRICE_CAD: "5.0"
          MIN_AVG_DOLLAR_VOLUME_CAD: "5000000"
          MAX_SCREEN_VOLATILITY: "0.80"
        run: |
          if [ -f "$MODEL_PATH" ]; then
            echo "Model manifest found at $MODEL_PATH"
          elif [ "$ALLOW_FALLBACK_TRAINING" = "1" ]; then
            echo "Model manifest missing; training a fallback model..."
            python -m stock_screener.cli train-model --log-level INFO
          else
            echo "Model manifest missing; skipping fallback training to protect runtime budget."
            echo "Daily run will continue and gracefully fall back from ML scoring if needed."
          fi

      - name: Run daily screener
        env:
          # Safety default to avoid runaway runs; you can override in repo variables if desired.
          MAX_TICKERS: "4000"
          TOP_N: "50"
          PORTFOLIO_SIZE: "8"
          DYNAMIC_SIZE_MAX_POSITIONS: "8"
          PORTFOLIO_BUDGET_CAD: "500"
          WEIGHT_CAP: "0.20"
          MIN_PRICE_CAD: "5.0"
          MIN_AVG_DOLLAR_VOLUME_CAD: "5000000"
          MAX_SCREEN_VOLATILITY: "0.80"
          # Entry quality filters (always on)
          ENTRY_MIN_CONFIDENCE: "0.5"
          ENTRY_MIN_PRED_RETURN: "0.01"
          ENTRY_MAX_VOLATILITY: "0.60"
          ENTRY_MIN_MOMENTUM_5D: "-0.05"
          ENTRY_DYNAMIC_THRESHOLDS_ENABLED: "1"
          ENTRY_DYNAMIC_MIN_CANDIDATES: "20"
          ENTRY_CONFIDENCE_PERCENTILE: "0.35"
          ENTRY_MIN_CONFIDENCE_FLOOR: "0.35"
          ENTRY_PRED_RETURN_PERCENTILE: "0.60"
          ENTRY_MIN_PRED_RETURN_FLOOR: "0.0025"
          # Use SOTA ML by default. If no model is present, the pipeline auto-falls back to baseline scoring.
          USE_ML: "1"
          MODEL_PATH: "models/ensemble/manifest.json"
          STRICT_FEATURE_PARITY: "1"
          APPLY_PREDICTION_RECALIBRATION: ${{ vars.APPLY_PREDICTION_RECALIBRATION || '1' }}
          MAX_DAILY_RUNTIME_MINUTES: "12"
          TURNOVER_PENALTY_BPS: "15"
          MIN_REBALANCE_WEIGHT_DELTA: "0.015"
          MIN_TRADE_NOTIONAL_CAD: "15"
          DYNAMIC_NO_TRADE_BAND_ENABLED: "1"
          DYNAMIC_NO_TRADE_UNCERTAINTY_WEIGHT: "1.2"
          DYNAMIC_NO_TRADE_LIQUIDITY_WEIGHT: "0.8"
          DYNAMIC_NO_TRADE_VOL_REGIME_WEIGHT: "0.8"
          DYNAMIC_NO_TRADE_MULTIPLIER_MIN: "1.0"
          DYNAMIC_NO_TRADE_MULTIPLIER_MAX: "3.0"
          # Quick-compounding: sell at predicted peak, redeploy capital fast
          MAX_HOLDING_DAYS: "3"
          MAX_HOLDING_DAYS_HARD: "5"
          QUICK_PROFIT_PCT: "0.03"
          QUICK_PROFIT_DAYS: "2"
          MIN_DAILY_RETURN: "0.008"
          LOW_DAILY_RETURN_HOLD_MIN_PRED_RETURN: "0.01"
          PORTFOLIO_STATE_PATH: "screener_portfolio_state.json"
          # Peak detection and partial exit settings
          PEAK_DETECTION_ENABLED: "1"
          PEAK_SELL_PORTION_PCT: "0.50"
          PEAK_MIN_GAIN_PCT: "0.04"
          PEAK_MIN_HOLDING_DAYS: "2"
          PEAK_PRED_RETURN_THRESHOLD: "-0.02"
          PEAK_SCORE_PERCENTILE_DROP: "0.30"
          PEAK_RSI_OVERBOUGHT: "70.0"
          PEAK_ABOVE_MA_RATIO: "0.15"
        run: |
          python -m stock_screener.cli daily --log-level INFO

      - name: Collect workflow telemetry
        if: always()
        shell: bash
        run: |
          END_TS=$(date +%s)
          START_TS=${WORKFLOW_START_TS:-$END_TS}
          ELAPSED_SEC=$((END_TS - START_TS))
          ELAPSED_MIN=$(awk "BEGIN {printf \"%.2f\", $ELAPSED_SEC/60.0}")
          CACHE_HITS=0
          TOTAL_CACHES=3
          [ "${{ steps.cache_pip.outputs.cache-hit }}" = "true" ] && CACHE_HITS=$((CACHE_HITS + 1))
          [ "${{ steps.cache_data_restore.outputs.cache-hit }}" = "true" ] && CACHE_HITS=$((CACHE_HITS + 1))
          [ "${{ steps.cache_model.outputs.cache-hit }}" = "true" ] && CACHE_HITS=$((CACHE_HITS + 1))
          CACHE_RATIO=$(awk "BEGIN {if($TOTAL_CACHES>0) printf \"%.3f\", $CACHE_HITS/$TOTAL_CACHES; else print \"0.000\"}")
          mkdir -p reports/telemetry
          cat > reports/telemetry/actions_telemetry.json <<JSON
          {
            "workflow": "daily-stock-screener",
            "run_id": "${{ github.run_id }}",
            "run_number": "${{ github.run_number }}",
            "elapsed_seconds": $ELAPSED_SEC,
            "elapsed_minutes": $ELAPSED_MIN,
            "minutes_estimate": $ELAPSED_MIN,
            "cache_hits": $CACHE_HITS,
            "cache_total": $TOTAL_CACHES,
            "cache_hit_ratio": $CACHE_RATIO
          }
          JSON

      - name: Compute workflow health counters
        if: always()
        id: health_counters
        shell: bash
        run: |
          python - <<'PY'
          import json
          import os
          from datetime import datetime, timezone
          from pathlib import Path


          def load_json(path: str):
              p = Path(path)
              if not p.exists():
                  return None
              try:
                  return json.loads(p.read_text(encoding="utf-8"))
              except Exception:
                  return None


          def parse_iso_utc(value):
              if not value:
                  return None
              s = str(value)
              if s.endswith("Z"):
                  s = f"{s[:-1]}+00:00"
              try:
                  dt = datetime.fromisoformat(s)
              except ValueError:
                  return None
              if dt.tzinfo is None:
                  dt = dt.replace(tzinfo=timezone.utc)
              return dt.astimezone(timezone.utc)


          telemetry = load_json("reports/telemetry/actions_telemetry.json") or {}
          state = load_json("screener_portfolio_state.json") or {}
          reward_log = load_json("cache/reward_log.json") or {}
          action_reward_log = load_json("cache/action_reward_log.json") or {}
          reward_policy = load_json("cache/reward_policy.json") or {}
          trade_actions = load_json("reports/trade_actions.json")
          if not isinstance(trade_actions, list):
              trade_actions = []

          positions = state.get("positions")
          if not isinstance(positions, list):
              positions = []
          open_positions = 0
          closed_positions = 0
          for position in positions:
              if not isinstance(position, dict):
                  continue
              status = str(position.get("status", "OPEN")).upper()
              if status == "OPEN":
                  open_positions += 1
              else:
                  closed_positions += 1

          last_updated = parse_iso_utc(state.get("last_updated"))
          if last_updated is None:
              state_age_hours = "na"
          else:
              age_hours = (datetime.now(timezone.utc) - last_updated).total_seconds() / 3600.0
              state_age_hours = f"{age_hours:.2f}"

          reward_entries = reward_log.get("entries") if isinstance(reward_log, dict) else None
          action_entries = action_reward_log.get("entries") if isinstance(action_reward_log, dict) else None
          reward_entry_count = len(reward_entries) if isinstance(reward_entries, list) else 0
          action_entry_count = len(action_entries) if isinstance(action_entries, list) else 0
          action_scored_count = 0
          if isinstance(action_entries, list):
              action_scored_count = sum(
                  1
                  for entry in action_entries
                  if isinstance(entry, dict) and entry.get("action_reward") is not None
              )

          policy_state = reward_policy.get("policy_state") if isinstance(reward_policy, dict) else {}
          if not isinstance(policy_state, dict):
              policy_state = {}
          policy_updates = int(policy_state.get("n_updates", 0) or 0)

          action_counts = {"BUY": 0, "SELL": 0, "SELL_PARTIAL": 0, "HOLD": 0}
          for action in trade_actions:
              if not isinstance(action, dict):
                  continue
              action_name = str(action.get("action", "")).upper()
              if action_name in action_counts:
                  action_counts[action_name] += 1

          outputs = {
              "elapsed_minutes": str(telemetry.get("elapsed_minutes", "na")),
              "cache_hits": str(telemetry.get("cache_hits", "na")),
              "cache_total": str(telemetry.get("cache_total", "na")),
              "cache_hit_ratio": str(telemetry.get("cache_hit_ratio", "na")),
              "open_positions": str(open_positions),
              "closed_positions": str(closed_positions),
              "state_age_hours": state_age_hours,
              "reward_entries": str(reward_entry_count),
              "action_reward_entries": str(action_entry_count),
              "action_reward_scored": str(action_scored_count),
              "policy_updates": str(policy_updates),
              "buy_count": str(action_counts["BUY"]),
              "sell_count": str(action_counts["SELL"]),
              "sell_partial_count": str(action_counts["SELL_PARTIAL"]),
              "hold_count": str(action_counts["HOLD"]),
          }

          with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as fh:
              for key, value in outputs.items():
                  fh.write(f"{key}={value}\n")
          PY

      - name: Write workflow summary
        if: always()
        run: |
          {
            echo "### Daily Screener Health Summary"
            echo ""
            echo "- Run status: \`${{ job.status }}\`"
            echo "- Runtime (minutes): \`${{ steps.health_counters.outputs.elapsed_minutes }}\`"
            echo "- Cache hit ratio: \`${{ steps.health_counters.outputs.cache_hit_ratio }}\` (\`${{ steps.health_counters.outputs.cache_hits }}/${{ steps.health_counters.outputs.cache_total }}\`)"
            echo "- Portfolio state age (hours): \`${{ steps.health_counters.outputs.state_age_hours }}\`"
            echo "- Open/closed positions: \`${{ steps.health_counters.outputs.open_positions }}/${{ steps.health_counters.outputs.closed_positions }}\`"
            echo "- Trade actions (BUY/SELL/SELL_PARTIAL/HOLD): \`${{ steps.health_counters.outputs.buy_count }}/${{ steps.health_counters.outputs.sell_count }}/${{ steps.health_counters.outputs.sell_partial_count }}/${{ steps.health_counters.outputs.hold_count }}\`"
            echo "- Reward log entries: \`${{ steps.health_counters.outputs.reward_entries }}\`"
            echo "- Action reward entries (scored): \`${{ steps.health_counters.outputs.action_reward_entries }} (${{ steps.health_counters.outputs.action_reward_scored }})\`"
            echo "- Reward policy updates: \`${{ steps.health_counters.outputs.policy_updates }}\`"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Save universe/data/state cache
        if: always()
        uses: actions/cache/save@v4
        with:
          path: |
            cache/
            data_cache/
            screener_portfolio_state.json
            screener_portfolio_state.json.bak
          key: ${{ runner.os }}-daily-screener-data-${{ hashFiles('requirements-actions.txt', '.github/workflows/daily-stock-screener.yml') }}-${{ github.run_id }}

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: daily-stock-screener-${{ github.run_number }}
          path: |
            reports/
            cache/last_run_meta.json
            screener_portfolio_state.json
          retention-days: 14

      - name: Determine market session
        id: session
        run: |
          HOUR=$(date -u +%H)
          if [ "$HOUR" -lt 13 ]; then
            echo "session=Pre-Market" >> $GITHUB_OUTPUT
          elif [ "$HOUR" -lt 17 ]; then
            echo "session=Mid-Day" >> $GITHUB_OUTPUT
          else
            echo "session=Pre-Close" >> $GITHUB_OUTPUT
          fi

      - name: Send daily email
        if: always()
        continue-on-error: true
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 465
          secure: true
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: "[${{ steps.session.outputs.session }}] Stock Screener + Portfolio (CAD)"
          to: ${{ secrets.EMAIL_TO || secrets.EMAIL_USERNAME }}
          from: Stock Screener Bot <${{ secrets.EMAIL_USERNAME }}>
          html_body: file://reports/daily_email.html
          attachments: reports/daily_report.txt,reports/portfolio_weights.csv,reports/trade_actions.json

      - name: Create GitHub Issue on Failure
        if: failure()
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            try {
              const issue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `Daily Stock Screener Failed - ${new Date().toISOString().split('T')[0]}`,
                body: `The daily stock screener failed.\n\n**Workflow URL:** ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}\n\nPlease check the run logs for details.`,
                labels: ['bug', 'automation', 'screener']
              });
              console.log(`Created issue ${issue.data.number}`);
            } catch (error) {
              console.log(`Could not create issue: ${error.message}`);
            }
